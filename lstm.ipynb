{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # 적절한 타이밍에 데이터 셰이프를 변환하거나, 데이터를 플롯하거나 표준화 가능\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('./data/train_x_df.csv')\n",
    "test_x = pd.read_csv('./data/test_x_df.csv')\n",
    "train_y = pd.read_csv('./data/train_y_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간값 구하는 함수\n",
    "def getMidprice(sample_id, df):\n",
    "  sample_id_df=df[df['sample_id']==sample_id]\n",
    "\n",
    "  high_prices = sample_id_df.loc[:, 'high'].values\n",
    "  low_prices = sample_id_df.loc[:, 'low'].values\n",
    "  mid_prices = (high_prices + low_prices)/2.0\n",
    "  \n",
    "  mid_prices=mid_prices.reshape(-1, 1) # scaler.fit_transform\n",
    "\n",
    "  return mid_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 검증 데이터셋 생성 함수\n",
    "# look_back : 관측값에서 유지해야 할 가격의 수, 예측전 지난 n개의 가격을 돌아보도록 함\n",
    "# foresight : 각 train 시퀀스에 대한 라벨이 시퀀스 다음 n+1분의 가격\n",
    "# dataset변수의 길이를 조정해서 넣어서 train 및 validation 각각 생성\n",
    "def create_train_dataset(dataset, look_back=210, foresight=119):\n",
    "  X, Y = [], []\n",
    "\n",
    "  for i in range(len(dataset)-look_back-foresight) :\n",
    "    # 관찰 값을 형성하는 특징으로 과거 210개의 가격 시퀀스 지정\n",
    "    obs=dataset[i:(i+look_back), 0]\n",
    "    # 시퀀스 추가\n",
    "    X.append(obs)\n",
    "    # 210개 가격의 한 시퀀스의 120분 후 가격 \n",
    "    Y.append(dataset[i+(look_back+foresight), 0])\n",
    "\n",
    "  return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 생성 함수\n",
    "def create_test_dataset(dataset, look_back=210):\n",
    "  x_test = []\n",
    "\n",
    "  for i in range(len(dataset)-look_back) :\n",
    "    # 관찰 값을 형성하는 특징으로 과거 210개의 가격 시퀀스 지정\n",
    "    obs=dataset[i:(i+look_back), 0]\n",
    "    # 시퀀스 추가\n",
    "    x_test.append(obs)\n",
    "\n",
    "  return np.array(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 및 예측\n",
    "- 대회에서 제공한 train dataset을 사용하지 않고 test 데이터만을 통해 예측했습니다.\n",
    "- 최대 예측값이 1보다 작으면 sell_time을 0으로 설정 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0it [00:00, ?it/s]0 / sample_id : 7661\nget mid price\nwindowed normalization\nExponential smoothing\nget train and validation data set\nreshaping\ntraining\n1it [00:53, 53.72s/it]1 / sample_id : 7662\nget mid price\nwindowed normalization\nExponential smoothing\nget train and validation data set\nreshaping\ntraining\n2it [01:44, 52.06s/it]2 / sample_id : 7663\nget mid price\nwindowed normalization\nExponential smoothing\nget train and validation data set\nreshaping\ntraining\n2it [02:12, 66.05s/it]\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-264e5319540f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;31m# 모델의 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   history = model.fit(x_train, y_train, validation_data=(x_validation, y_validation), \n\u001b[0;32m---> 85\u001b[0;31m                       epochs=50, batch_size=3000, verbose=0, callbacks=[early_stopping_callback])\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samples=test_x['sample_id'].unique()\n",
    "pred_selling_time = []\n",
    "\n",
    "for idx, id in tqdm(enumerate(samples)):\n",
    "  print(idx, \"/ sample_id :\", id)\n",
    "  print(\"get mid price\")\n",
    "  # sample_id에 대한 중간값으로 학습 및 테스트 데이터 생성\n",
    "  x_mid_prices = getMidprice(id, test_x)\n",
    "\n",
    "  ##############################################################################\n",
    "\n",
    "  print(\"windowed normalization\")\n",
    "  ### 윈도우 방식 표준화\n",
    "  # 데이터를 표준화하기 위한 윈도우 크기\n",
    "  normalization_window = 138\n",
    "\n",
    "  # 표준화 범위\n",
    "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "  x_win_prices=x_mid_prices.copy()\n",
    "  # 한 번에 138개의 x_train 데이터별로 윈도우 표준화\n",
    "  for i in range(0, 1380, normalization_window):\n",
    "    # 현재 윈도우에 대해 스케일러 객체를 데이터에 적용\n",
    "    scaler.fit(x_mid_prices[i:i+normalization_window, :])\n",
    "    # 현재 윈도우의 데이터를 선택한 특징 범위(0~1)의 데이터로 변환\n",
    "    x_win_prices[i:i+normalization_window,:] = scaler.transform(x_mid_prices[i:i+normalization_window,:])\n",
    "\n",
    "  ##############################################################################\n",
    "\n",
    "  print(\"Exponential smoothing\")\n",
    "  # 지수 평활법 - 최근 사건이 과거보다 현재 데이터에 더 많은 영향을 주도록\n",
    "  # 데이터에 포함된 고주파 노이즈 제거\n",
    "\n",
    "  Smoothing=0 # 평활화 값을 0으로 초기화\n",
    "  gamma = 0.3 # 소멸 계수\n",
    "  x_sm_prices=[]\n",
    "\n",
    "  # x_train데이터 평활화\n",
    "  for a in range(1380):\n",
    "    # 평활화 값 업데이트\n",
    "    Smoothing = gamma*x_win_prices[a]+(1-gamma)*Smoothing\n",
    "    # 평활화 값으로 데이터 포인트 값을 대체\n",
    "    x_sm_prices.append(Smoothing)\n",
    "\n",
    "  x_sm_prices=np.array(x_sm_prices).reshape(-1, 1)\n",
    "\n",
    "  ##############################################################################\n",
    "\n",
    "  print(\"get train and validation data set\")\n",
    "  x_train, y_train = createTrainValid(x_sm_prices[:1259])\n",
    "  x_validation, y_validation = createTrainValid(x_sm_prices)\n",
    "  x_test = createTestset(x_sm_prices)\n",
    "  \n",
    "  ##############################################################################\n",
    "\n",
    "  print(\"reshaping\")\n",
    "  x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "  y_train = np.reshape(y_train, (y_train.shape[0], 1, 1)) \n",
    "\n",
    "  x_validation = np.reshape(x_validation, (x_validation.shape[0], 1, x_validation.shape[1]))\n",
    "  y_validation = np.reshape(y_validation, (y_validation.shape[0], 1, 1)) \n",
    "\n",
    "  x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "  ##############################################################################\n",
    "\n",
    "  print(\"training\")\n",
    "  # seed 값 설정\n",
    "  seed = 0\n",
    "  np.random.seed(seed)\n",
    "  tf.random.set_seed(3)\n",
    "\n",
    "  model=Sequential()\n",
    "  model.add(LSTM(480, input_shape=(1, 210), dropout=0.1, recurrent_dropout=0.2, return_sequences=True))\n",
    "  model.add(LSTM(480, dropout=0.1, recurrent_dropout=0.2, return_sequences=True))\n",
    "  model.add(LSTM(480, dropout=0.1, recurrent_dropout=0.2))\n",
    "  model.add(Dense(1, activation='linear'))\n",
    "\n",
    "  model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "  early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "  # 모델의 실행\n",
    "  history = model.fit(x_train, y_train, validation_data=(x_validation, y_validation), \n",
    "                      epochs=50, batch_size=3000, verbose=0, callbacks=[early_stopping_callback])\n",
    "\n",
    "  preds = model.predict(x_test)[-120:]\n",
    "  preds = preds.flatten()\n",
    "\n",
    "  st = 0 \n",
    "  if scaler.inverse_transform(preds.max().reshape(-1,1))/x_mid_prices[-1,0] > 1:\n",
    "    st = preds.argmax()\n",
    "  pred_selling_time.append(st)\n",
    "\n",
    "pred_selling_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['sample_id'] = range(x_train_open.shape[0], x_train_open.shape[0]+x_test_open.shape[0])\n",
    "submission['buy_quantity'] = 1\n",
    "submission['sell_time'] = pred_selling_time\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission.to_csv('submission_lstm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}